{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7f5cef-f3f5-4422-bf83-078d715980c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/lucas/Attention-Is-All-You-Need/\")\n",
    "\n",
    "from transformer import TransformerFromScratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99db9c2e-396d-44d6-a97a-5d46c7ab63b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfomer_from_scratch = TransformerFromScratch(3,4,4,120,512,64,64,4,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a849f48f-7e5e-4bf7-9f7e-cdc62a9a98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(2,3,512)\n",
    "tgt_tensor = torch.randn(2,3,512)\n",
    "\n",
    "mask = torch.triu(torch.ones((3,3),dtype=torch.int8)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "271101de-eeab-4634-88e8-e56a43065472",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = transfomer_from_scratch(input_tensor,tgt_tensor,tgt_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e3930ab-66cf-45a1-8c27-7bccf5ed8157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 120])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f924d598-6efd-4a92-b7c3-d9b09f12b770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "         2, 0, 0, 0, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
       "         2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 2, 2, 0, 0, 0, 0,\n",
       "         0, 0, 0, 2, 2, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 2, 2,\n",
       "         0, 2, 0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 2,\n",
       "         2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 0, 2,\n",
       "         0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.argmax(dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d215aadd-c139-455a-a61e-1e2c3ad4f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(3,4,512,64,64,4,2048)\n",
    "decoder = Decoder(3,4,512,64,64,4,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60869958-f185-434d-a698-ad124f12526b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b11401e-6d89-457e-93cd-f99d58459603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [1, 1, 0],\n",
       "        [1, 1, 1]], dtype=torch.int8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26d1c4f5-aa71-40b9-9347-cb6dfeafc5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encoder(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "419f3db2-7222-492c-bbd1-a6f67b100dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "output = decoder(tgt_tensor, x, tgt_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e70ce9a-2b87-47c4-a7b3-7c1faf9c1113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89ed2046-c8df-4362-962f-dd14c86f6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = decoder(tgt_tensor, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44452800-e544-4e90-a221-d0fda291341a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5692, -0.3596,  0.3554,  ..., -0.3345, -0.9155,  0.1720],\n",
       "         [-0.5692, -0.3596,  0.3554,  ..., -0.3345, -0.9155,  0.1720],\n",
       "         [-0.5692, -0.3596,  0.3554,  ..., -0.3345, -0.9155,  0.1720]],\n",
       "\n",
       "        [[ 0.0579, -1.1439,  1.4661,  ..., -0.5045, -0.1635, -1.2986],\n",
       "         [ 0.0579, -1.1439,  1.4661,  ..., -0.5045, -0.1635, -1.2986],\n",
       "         [ 0.0579, -1.1439,  1.4661,  ..., -0.5045, -0.1635, -1.2986]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29a93e36-d919-4e7a-b85c-60c2ea53cf51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5692, -0.3596,  0.3554,  ..., -0.3345, -0.9155,  0.1720],\n",
       "         [-0.5692, -0.3596,  0.3554,  ..., -0.3345, -0.9155,  0.1720],\n",
       "         [-0.5692, -0.3596,  0.3554,  ..., -0.3345, -0.9155,  0.1720]],\n",
       "\n",
       "        [[ 0.0579, -1.1439,  1.4661,  ..., -0.5045, -0.1635, -1.2986],\n",
       "         [ 0.0579, -1.1439,  1.4661,  ..., -0.5045, -0.1635, -1.2986],\n",
       "         [ 0.0579, -1.1439,  1.4661,  ..., -0.5045, -0.1635, -1.2986]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad6ebc-8b1e-4b9d-8a05-630c625b4282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a739d018-3c26-45cd-bb68-03cbdabb5451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b23a61-7e9a-44a5-96da-a88a91600da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, seq_length:int, d_model:int, n = 10000, dtype = torch.float32):\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.d_model = d_model\n",
    "        self.n = n\n",
    "        self.dtype = dtype\n",
    "        self.encode_table = self._create_table()\n",
    "\n",
    "    def _create_table(self) -> torch.tensor:\n",
    "        table = torch.zeros((self.seq_length, self.d_model),dtype=self.dtype)\n",
    "\n",
    "        for pos in torch.arange(self.seq_length):\n",
    "            # Here d_model is divide by 2 because the 2*i and 2*i + 1\n",
    "            # The interaction foward two by two \n",
    "            for i in torch.arange(self.d_model//2):\n",
    "                denominator = 2*i/self.d_model\n",
    "                calculation = pos/torch.pow(self.n, denominator)\n",
    "                table[pos,2*i]      = torch.sin(calculation) \n",
    "                table[pos,2*i + 1]  = torch.cos(calculation)\n",
    "\n",
    "        return table\n",
    "\n",
    "    def forward(self,x:torch.tensor) -> torch.tensor:\n",
    "        x += self.encode_table\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e80408a-807e-427e-9522-dab21ba38351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query:torch.tensor, key:torch.tensor, value:torch.tensor, mask=None) -> [torch.tensor,torch.tensor]:\n",
    "    factor = 1/torch.sqrt(torch.tensor(key.size(-1)))\n",
    "    attn = torch.matmul(query,key.transpose(-2,-1))*factor\n",
    "    if(mask is not None):\n",
    "        attn = attn.masked_fill(mask == 0, -torch.inf)\n",
    "    attn = F.softmax(attn,dim=-1)\n",
    "    \n",
    "    x = torch.matmul(attn,value)\n",
    "\n",
    "    return x, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71cc56bf-4b10-4cc0-8188-aaca745efced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadAttention(nn.Module):\n",
    "    def __init__(self, d_model:int, d_k:int, d_v:int):\n",
    "        super(HeadAttention,self).__init__()\n",
    "        self.weights_query = nn.Parameter(torch.randn(d_model,d_k))\n",
    "        self.weights_key = nn.Parameter(torch.randn(d_model,d_k))\n",
    "        self.weights_value = nn.Parameter(torch.randn(d_model,d_v))\n",
    "\n",
    "    def forward(self, query:torch.tensor, key:torch.tensor, value:torch.tensor, mask=None) -> torch.tensor:\n",
    "        q = torch.matmul(query,self.weights_query)\n",
    "        k = torch.matmul(key,self.weights_key)\n",
    "        v = torch.matmul(value,self.weights_value)\n",
    "\n",
    "        x, _ = scaled_dot_product_attention(q,k,v,mask=mask)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0b7211d-3c55-4521-ba21-02966d9853eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, d_model:int, d_k:int, d_v:int, h:int):\n",
    "        super(MultiHead,self).__init__()\n",
    "        self.weights_concat = nn.Parameter(torch.randn(d_v*h,d_model))\n",
    "\n",
    "        self.multi_head = nn.ModuleList([\n",
    "            HeadAttention(d_model=d_model, d_k=d_k, d_v=d_v) for _ in range(h)\n",
    "        ])\n",
    "\n",
    "        self.norm_layer = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, query:torch.tensor, key:torch.tensor, value:torch.tensor, mask=None) -> torch.tensor:\n",
    "        x = torch.concat([module_(query, key, value, mask=mask) for module_ in self.multi_head],dim=-1)\n",
    "        x = torch.matmul(x,self.weights_concat)\n",
    "\n",
    "        x += query\n",
    "\n",
    "        x = self.norm_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64986463-ba99-4549-bce7-d4c88c9333fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForwardNetworks(nn.Module):\n",
    "    def __init__(self, d_model:int, d_ff:int):\n",
    "        super(PositionWiseFeedForwardNetworks,self).__init__()\n",
    "        self.linear_inner_layer = nn.Linear(d_model,d_ff)\n",
    "        self.linear_layer = nn.Linear(d_ff,d_model)\n",
    "\n",
    "        self.norm_layer = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x:torch.tensor) -> torch.tensor:\n",
    "        x = self.linear_inner_layer(x)\n",
    "        x = x.relu()\n",
    "        x = self.linear_layer(x)\n",
    "\n",
    "        x += x\n",
    "        x = self.norm_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d236360-6458-4bfb-a07f-1d065fc0e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model:int, d_k:int, d_v:int, h:int, d_ff:int):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.multi_head_attention = MultiHead(d_model=d_model,d_k=d_k,d_v=d_v,h=h)\n",
    "        self.position_wise_feed_forward_networks = PositionWiseFeedForwardNetworks(d_model=d_model,d_ff=d_ff)\n",
    "\n",
    "    def forward(self, query:torch.tensor, key:torch.tensor, value:torch.tensor, mask=None) -> torch.tensor:\n",
    "        x = self.multi_head_attention(query=value, key=key, value=value, mask=mask)\n",
    "        x = self.position_wise_feed_forward_networks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f3f6db9-1d05-4be8-9b70-ae27d35c1ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, seq_length: int, n_layers:int, d_model:int, d_k:int, d_v:int, h:int, d_ff:int):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(seq_length=seq_length,d_model=d_model)\n",
    "        self.encoder_layer = nn.ModuleList([\n",
    "            EncoderLayer(d_model=d_model,d_k=d_k,d_v=d_v,h=h,d_ff=d_ff) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, src:torch.tensor, mask=None) -> torch.tensor:\n",
    "        src = self.positional_encoding(src)\n",
    "        for module_ in self.encoder_layer:\n",
    "            src += module_(src,src,src,mask)\n",
    "\n",
    "        return src\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c85e1df-d5b6-4856-8e36-359c3ce5c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model:int, d_k:int, d_v:int, h:int, d_ff:int):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.multi_head_attention = MultiHead(d_model=d_model,d_k=d_k,d_v=d_v,h=h)\n",
    "        self.multi_head_attention_memory = MultiHead(d_model=d_model,d_k=d_k,d_v=d_v,h=h)\n",
    "        self.position_wise_feed_forward_networks = PositionWiseFeedForwardNetworks(d_model=d_model,d_ff=d_ff)\n",
    "\n",
    "    def forward(self, query:torch.tensor, key:torch.tensor, value:torch.tensor, memory:torch.tensor, mask=None) -> torch.tensor:\n",
    "        x = self.multi_head_attention(query=value, key=key, value=value, mask=mask)\n",
    "        x = self.multi_head_attention_memory(query=x, key=memory, value=memory)\n",
    "        x = self.position_wise_feed_forward_networks(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "726aa285-2fd0-44f5-b224-629d01615212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, seq_length: int, n_layers:int, d_model:int, d_k:int, d_v:int, h:int, d_ff:int):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(seq_length=seq_length,d_model=d_model)\n",
    "        self.encoder_layer = nn.ModuleList([\n",
    "            DecoderLayer(d_model=d_model,d_k=d_k,d_v=d_v,h=h,d_ff=d_ff) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, src:torch.tensor, memory:torch.tensor, mask=None) -> torch.tensor:\n",
    "        src = self.positional_encoding(src)\n",
    "        for module_ in self.encoder_layer:\n",
    "            src += module_(src,src,src,memory,mask)\n",
    "\n",
    "        return src\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c05da7-5090-4315-b86c-203171454f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(3,4,512,64,64,4,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07964770-d015-45da-add7-99e716b07ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(3,4,512,64,64,4,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b0575ee-8d62-4fdb-8127-342ab6cd1d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(2,3,512)\n",
    "tgt_tensor = torch.randn(2,3,512)\n",
    "\n",
    "mask = torch.triu(torch.ones((3,3),dtype=torch.int8)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5abaa02f-a0de-4e91-9211-961549b7189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encoder(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dab8a560-96c4-4b34-af20-87fdf625a8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "772f4e07-0e21-445a-b7ec-676c3f7f798b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6582, -0.8472,  1.2657,  ...,  0.1043,  1.1291, -0.4406],\n",
       "         [-1.6582, -0.8472,  1.2657,  ...,  0.1043,  1.1291, -0.4406],\n",
       "         [-1.6582, -0.8472,  1.2657,  ...,  0.1043,  1.1291, -0.4406]],\n",
       "\n",
       "        [[-1.3110, -1.6264,  0.0674,  ...,  0.9752,  1.7127,  0.5505],\n",
       "         [-1.3110, -1.6264,  0.0674,  ...,  0.9752,  1.7127,  0.5505],\n",
       "         [-1.3110, -1.6264,  0.0674,  ...,  0.9752,  1.7127,  0.5505]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(tgt_tensor,tgt_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a180cb-6ac6-4d20-9065-2f22ce818976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d39e186-e480-43c0-815c-33f02e5f46d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa21b0-4374-4977-934a-c27a06d3d469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "50518658-cee8-436b-9756-fb613a3da38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5c2b47cd-ba8e-4df1-9979-5a3c5db7e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.triu(torch.ones((3,3),dtype=torch.int8)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "14f26e38-45b9-40ff-8b0a-75fb5ee1468c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0219, -1.3973, -0.3051,  ...,  2.0317, -0.6028,  5.2137],\n",
       "         [ 3.6245, -0.2907,  1.7433,  ...,  0.3274,  0.8121,  7.7216],\n",
       "         [-1.1843, -1.7584,  1.5053,  ...,  1.3223,  2.6765,  6.5351]],\n",
       "\n",
       "        [[-2.4180, -4.9422,  3.7338,  ...,  3.6649,  1.1594,  4.1164],\n",
       "         [-0.7324, -3.0181,  2.3415,  ...,  1.3145,  2.9303,  3.7378],\n",
       "         [ 1.0661, -1.7807,  4.3706,  ..., -0.2056, -0.0184,  4.5398]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(input_tensor,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a095b9-2dd9-476c-95d6-5f9fc19ccfdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
